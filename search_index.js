var documenterSearchIndex = {"docs":
[{"location":"log_posterior_background/#Background:-Logarithmic-posterior-probability","page":"Background","title":"Background: Logarithmic posterior probability","text":"","category":"section"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"The general setting is the same as in Background:-Posterior-probability. The starting point is","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"p(lambda mid x_i_i=1^N y_i_i=1^N m) propto  p_0(lambdamid x_i_i=1^N m) prod_i_1^n ell_i(y_imid lambda x_im) ","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"assuming the likelihoods ell_i are known.","category":"page"},{"location":"log_posterior_background/#Product-of-small-numbers","page":"Background","title":"Product of small numbers","text":"","category":"section"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"In the formula for the posterior likelihood, it can happen that many small numbers (close to zero) need to be multiplied together. Because floating point numbers can only represent numbers up to a certain precision, such products, though theoretically non-zero, tend to be rounded to zero.","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"For example, consider the following array as the likelihood values:","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"using Distributions, BenchmarkTools\nsmall_values = [pdf(Normal(0,1),10+i) for i in 1:10]","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"Although the values are non-zero, the product is rounded to zero:","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"prod(small_values)","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"One could use floating point types with higher precision:","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"small_values_high_precision = BigFloat.(small_values)\nprod(small_values_high_precision)","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"However, this entails a huge performance loss together with increased memory usage: ","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"@benchmark prod(small_values)","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"@benchmark prod(small_values_high_precision)","category":"page"},{"location":"log_posterior_background/#Logarithmic-scale","page":"Background","title":"Logarithmic scale","text":"","category":"section"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"Since posterior probabilities are often unnormalized anyways, one is not interested in the particular values, but only in relative differences. But then, any strictly monotonic function can be applied to compare relative differences. A convenient choice for such a strictly monotonic (increasing) function is the logarithm.","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"Note that because of proportionality, there is a constant alpha  0 such that","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"p(lambda mid x_i_i=1^N y_i_i=1^N m) =  alpha cdot  p_0(lambdamid x_i_i=1^N m) prod_i_1^n ell_i(y_imid lambda x_im) ","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"Applying the natural logarithm leads to:","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"beginaligned\nln (p(lambda mid x_i_i=1^N y_i_i=1^N m)) =  ln left(alpha cdot  p_0(lambdamid x_i_i=1^N m) prod_i_1^n ell_i(y_imid lambda x_im) right)  \n= ln(p_0(lambdamid x_i_i=1^N m)) + sum_i=1^N ln(ell_i(y_imid lambda x_im)) + ln(alpha)\nendaligned","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"Using the logarithm allowed to exchange the multiplication of small numbers for an addition in the logarithmic scale, at the cost of having to calculate the logarithm of every value. However, the cost of calculating the logarithm is the worst case scenario. In many cases, it is possible if not easier to implement logarithms of the involved densities (e.g. for the normal distribution, laplace distribution, etc.).","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"To shorten the notation, denote the logarithms of the distributions by L_p = ln circ p for the posterior, L_i= ln circ ell_i for the likelihoods and L_0 =ln circ p_0 for the prior: ","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"L_p(lambda mid x_i_i=1^N y_i_i=1^N m) =   L_0(lambdamid x_i_i=1^N m) +  sum_i_1^n L_i(y_imid lambda x_im) + textconst","category":"page"},{"location":"log_posterior_background/#Effect-of-Logarithmic-scale","page":"Background","title":"Effect of Logarithmic scale","text":"","category":"section"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"Using the logarithm has two effects. First of all, the product becomes a sum. This alone would suffice to prevent the rounding to zero problem:","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"sum(small_values)","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"In addition, the logarithm has the effect of compressing the number scale for numbers larger than 1 and to stretch out the number scale for numbers between 0 and 1:","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"log_values = log.(small_values)","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"Of course, the sum is still non-zero:","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"sum(log_values)","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"While the use of higher precision floating point numbers (BigFloat) meant a huge performance loss, the log scale method dose not impair performance:","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"@benchmark sum(log_values)","category":"page"},{"location":"log_posterior_background/#Logarithmic-density-example","page":"Background","title":"Logarithmic density example","text":"","category":"section"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"In Logarithmic-scale it was mentioned that some distributions are even easier to be implemented in a logarithmic scale. This is not only the case for the definition of densities from scratch, but also applies for Distributions.jl. Observe that \"far\" away from the mean, the pdf of a normal distribution is rounded to zero:","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"pdf(Normal(0,1),100)","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"Obviously, the logarithm cannot be applied to this. However, Distributions.jl offers a logpdf function:","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"logpdf(Normal(0,1),100)","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"This allows values even further away from the mean:","category":"page"},{"location":"log_posterior_background/","page":"Background","title":"Background","text":"logpdf(Normal(0,1),10^20)","category":"page"},{"location":"log_posterior_implementation/#Logarithmic-posterior-probability:-How-to-implement","page":"How to implement","title":"Logarithmic posterior probability: How to implement","text":"","category":"section"},{"location":"log_posterior_implementation/","page":"How to implement","title":"How to implement","text":"Consider the data and model from Simple-example:","category":"page"},{"location":"log_posterior_implementation/","page":"How to implement","title":"How to implement","text":"using FittingObjectiveFunctions, Plots #hide\n\nX = collect(1:10)\nY = [1.0, 1.78, 3.64, 3.72, 5.33, 2.73, 7.52, 9.19, 6.73, 8.95]\nΔY = [0.38, 0.86, 0.29, 0.45, 0.66, 2.46, 0.39, 0.45, 1.62, 1.54]\nmodel = ModelFunctions((x,λ)-> λ*x)\n\nnothing #hide","category":"page"},{"location":"log_posterior_implementation/#Log-likelihood-and-log-posterior","page":"How to implement","title":"Log-likelihood and log-posterior","text":"","category":"section"},{"location":"log_posterior_implementation/","page":"How to implement","title":"How to implement","text":"The general procedure to obtain the log-likelihood and log-posterior functions is the same as described in Posterior-probability:-How-to-implement. However, the distributions and the prior need to be in logarithmic form, such that the default distributions of the FittingData constructor do not work.  Thus, we need to define a FittingData object with logarithmic distributions:","category":"page"},{"location":"log_posterior_implementation/","page":"How to implement","title":"How to implement","text":"using Distributions\ndata_log_dists = FittingData(X,Y,ΔY,distributions = (y,m,Δy)-> logpdf(Normal(m,Δy),y))\nnothing #hide","category":"page"},{"location":"log_posterior_implementation/","page":"How to implement","title":"How to implement","text":"The log-likelihood function can be obtained by using the log_posterior_objective function:","category":"page"},{"location":"log_posterior_implementation/","page":"How to implement","title":"How to implement","text":"log_likelihood = log_posterior_objective(data_log_dists,model)\nnothing #hide","category":"page"},{"location":"log_posterior_implementation/","page":"How to implement","title":"How to implement","text":"As described in Using-priors, the likelihood is obtained by using the prior λ-> 1 (or in the logarithmic case λ-> 0). Again, this is what happens in the background. To use prior, it just needs to be passed as third argument:","category":"page"},{"location":"log_posterior_implementation/","page":"How to implement","title":"How to implement","text":"log_posterior = log_posterior_objective(data_log_dists,model, λ-> logpdf(Normal(1,0.1),λ))\nnothing #hide","category":"page"},{"location":"log_posterior_implementation/","page":"How to implement","title":"How to implement","text":"The resulting functions can be compared by adjusting the constant offset (see Logarithmic scale)","category":"page"},{"location":"log_posterior_implementation/","page":"How to implement","title":"How to implement","text":"large_scope = plot(x-> log_likelihood(x) + 1.105, xlims = [0.9,1.2], label = \"log_likelihood\", legend = :topleft) #hide\nplot!(log_posterior, label = \"log_posterior\")  #hide\nsmall_scope = plot(x-> log_likelihood(x) + 1.105, xlims = [1.065,1.085], legend = :none) #hide\nplot!(log_posterior)  #hide\nplot(large_scope,small_scope, layout = (1,2), size = (800,300)) #hide","category":"page"},{"location":"log_posterior_implementation/#Application:-Regularized-least-squares","page":"How to implement","title":"Application: Regularized least squares","text":"","category":"section"},{"location":"log_posterior_implementation/","page":"How to implement","title":"How to implement","text":"Recall the weighted least squares objective from LSQ:-How-to-implement","category":"page"},{"location":"log_posterior_implementation/","page":"How to implement","title":"How to implement","text":"lsq = lsq_objective(data_log_dists, model)\nnothing #hide","category":"page"},{"location":"log_posterior_implementation/","page":"How to implement","title":"How to implement","text":"Note that the distributions field of data_log_dists has no effect on least squares objectives. ","category":"page"},{"location":"log_posterior_implementation/","page":"How to implement","title":"How to implement","text":"To replicate the least squares objective, unnormalized logarithmic distributions can be used:","category":"page"},{"location":"log_posterior_implementation/","page":"How to implement","title":"How to implement","text":"data_lsq_dists = FittingData(X,Y,ΔY, distributions = (y,m,Δy)-> -(y-m)^2/Δy^2)\nlsq_likelihood = log_posterior_objective(data_lsq_dists,model)\nnothing #hide","category":"page"},{"location":"log_posterior_implementation/","page":"How to implement","title":"How to implement","text":"plot(lsq, label = \"lsq\") #hide\nplot!(lsq_likelihood, label = \"lsq_likelihood\") #hide","category":"page"},{"location":"log_posterior_implementation/","page":"How to implement","title":"How to implement","text":"The lsq_likelihood is the same function as lsq, but with the opposite sign (because it is a logarithmic unnormalized posterior probability density). This could either be fixed by using λ -> -lsq_likelihood(λ), or in this case by using distributions = (y,m,Δy)-> -(y-m)^2/Δy^2.","category":"page"},{"location":"log_posterior_implementation/","page":"How to implement","title":"How to implement","text":"Now, a regularization can be implemented by using a corresponding logarithmic prior:","category":"page"},{"location":"log_posterior_implementation/","page":"How to implement","title":"How to implement","text":"lsq_posterior = log_posterior_objective(data_lsq_dists,model, λ -> - λ^2)\nnothing #hide","category":"page"},{"location":"log_posterior_implementation/#Derivatives","page":"How to implement","title":"Derivatives","text":"","category":"section"},{"location":"log_posterior_implementation/","page":"How to implement","title":"How to implement","text":"Analytical derivatives can be obtained almost in the same way as described in LSQ:-partial-derivatives-and-gradients:","category":"page"},{"location":"log_posterior_implementation/#Partial-derivatives","page":"How to implement","title":"Partial derivatives","text":"","category":"section"},{"location":"log_posterior_implementation/","page":"How to implement","title":"How to implement","text":"@doc log_posterior_partials #hide","category":"page"},{"location":"log_posterior_implementation/#Gradient","page":"How to implement","title":"Gradient","text":"","category":"section"},{"location":"log_posterior_implementation/","page":"How to implement","title":"How to implement","text":"@doc log_posterior_gradient #hide","category":"page"},{"location":"fitting_data/#FittingData-and-ModelFunctions","page":"FittingData and ModelFunctions","title":"FittingData and ModelFunctions","text":"","category":"section"},{"location":"fitting_data/","page":"FittingData and ModelFunctions","title":"FittingData and ModelFunctions","text":"using FittingObjectiveFunctions #hide\nmodel = ModelFunctions((x,λ) -> λ*x) #hide\nX = collect(1:10) #hide\nY = [1.0, 1.78, 3.64, 3.72, 5.33, 2.73, 7.52, 9.19, 6.73, 8.95] #hide\nΔY = [0.38, 0.86, 0.29, 0.45, 0.66, 2.46, 0.39, 0.45, 1.62, 1.54] #hide\nnothing #hide","category":"page"},{"location":"fitting_data/","page":"FittingData and ModelFunctions","title":"FittingData and ModelFunctions","text":"In the Simple-example, it was already mentioned that the data needs to be summarized in a FittingData object and that information about the model needs to be summarized in a ModelFunctions object. The details about these data types are discussed here.","category":"page"},{"location":"fitting_data/#FittingData","page":"FittingData and ModelFunctions","title":"FittingData","text":"","category":"section"},{"location":"fitting_data/","page":"FittingData and ModelFunctions","title":"FittingData and ModelFunctions","text":"FittingData objects have the following general constructor:","category":"page"},{"location":"fitting_data/","page":"FittingData and ModelFunctions","title":"FittingData and ModelFunctions","text":"data = FittingData(X,Y,ΔY, distributions = distribution_functions)","category":"page"},{"location":"fitting_data/","page":"FittingData and ModelFunctions","title":"FittingData and ModelFunctions","text":"The resulting object has the following fields:","category":"page"},{"location":"fitting_data/","page":"FittingData and ModelFunctions","title":"FittingData and ModelFunctions","text":"data.independent == X\ndata.dependent == Y\ndata.errors == ΔY\ndata.distributions == distributions_functions","category":"page"},{"location":"fitting_data/","page":"FittingData and ModelFunctions","title":"FittingData and ModelFunctions","text":"tip: Tip: shortened constructors\nIf no measurement errors ΔY are available, one can use the shortened constructorFittingData(X,Y, distributions = distribution_functions)In this case, the default errors ones(length(X)) are used (leading e.g. to the standard least squares objective function: Background:-LSQ)","category":"page"},{"location":"fitting_data/","page":"FittingData and ModelFunctions","title":"FittingData and ModelFunctions","text":"tip: Tip: distributions\nThe optional distributions keyword is used to specify the likelihood distributions (see Background:-Posterior-probability). In case distributions are not specified, the constructor defaults to normal distributions.The distributions can be specified as an array of functions, or a single function (if the same distribution shall be used for all data points) with the signature (y,m,Δy), wherey is the measured dependent variable\nΔy is the corresponding error\nm are values the model function returns, when the parameters are varied, i.e. m(x,λ).","category":"page"},{"location":"fitting_data/#ModelFunctions","page":"FittingData and ModelFunctions","title":"ModelFunctions","text":"","category":"section"},{"location":"fitting_data/","page":"FittingData and ModelFunctions","title":"FittingData and ModelFunctions","text":"ModelFunctions objects have the following general constructor:","category":"page"},{"location":"fitting_data/","page":"FittingData and ModelFunctions","title":"FittingData and ModelFunctions","text":"model = ModelFunctions(model_function, partial_derivatives = [derivative_functions...])","category":"page"},{"location":"fitting_data/","page":"FittingData and ModelFunctions","title":"FittingData and ModelFunctions","text":"The resulting object has the following fields:","category":"page"},{"location":"fitting_data/","page":"FittingData and ModelFunctions","title":"FittingData and ModelFunctions","text":"model.model == model_function\nmodel.partials == [derivative_functions...]","category":"page"},{"location":"fitting_data/","page":"FittingData and ModelFunctions","title":"FittingData and ModelFunctions","text":"The model function (and the partial derivatives) must have the signature (x,λ), where x is the independent variable and λ is the parameter(array).","category":"page"},{"location":"fitting_data/","page":"FittingData and ModelFunctions","title":"FittingData and ModelFunctions","text":"tip: Tip: Partial derivatives\nThe keyword partials is optional, but is required for analytical partial derivatives and analytical gradient functions. The partial derivatives w.r.t. to the parameter are defined as array of functions.textm(xlambda) = lambda_1 x + lambda_2  quad fracpartial textm(xlambda)partial lambda_1  = x  quad fracpartial textm(xlambda)partial lambda_1 = 1ModelFunctions((x,λ)-> λ[1]*x + λ[2], partials = [(x,λ)-> x, (x,λ)-> 1])","category":"page"},{"location":"fitting_data/#Additional-remarks","page":"FittingData and ModelFunctions","title":"Additional remarks","text":"","category":"section"},{"location":"fitting_data/","page":"FittingData and ModelFunctions","title":"FittingData and ModelFunctions","text":"When a FittingObject/ModelFunctions object is created, some rudimentary consistency checks are made, e.g. that all arrays have the same lengths.","category":"page"},{"location":"fitting_data/","page":"FittingData and ModelFunctions","title":"FittingData and ModelFunctions","text":"FittingData([1,2,3],[1,2],[1,1,1])","category":"page"},{"location":"fitting_data/","page":"FittingData and ModelFunctions","title":"FittingData and ModelFunctions","text":"Since the objects are mutable, the same consistency checks are repeated before objective functions are created. However, it can be useful to make a comprehensive consistency check. For this, an exemplary parameter (array) is needed:","category":"page"},{"location":"fitting_data/","page":"FittingData and ModelFunctions","title":"FittingData and ModelFunctions","text":"data = FittingData(X,Y)\nmodel = ModelFunctions((x,λ)-> λ*x)\nconsistency_check(data,model,1)","category":"page"},{"location":"fitting_data/","page":"FittingData and ModelFunctions","title":"FittingData and ModelFunctions","text":"If everything works, nothing is returned, i.e. nothing happens. However, in case of a problem, an error is thrown:","category":"page"},{"location":"fitting_data/","page":"FittingData and ModelFunctions","title":"FittingData and ModelFunctions","text":"consistency_check(data,model,\"1\")","category":"page"},{"location":"fitting_data/","page":"FittingData and ModelFunctions","title":"FittingData and ModelFunctions","text":"note: Mutability of objects\nBoth FittingData objects and ModelFunction objects are mutable for convenience, as they are not performance relevant (only their fields are). However, when objective functions are created, the object fields are copied and enclosed in the objective function, to avoid accidental mutation. I.e. once an objective function is created, it does not change, even if the FittingData/ModelFunctions objects are changed. To apply changes, a new objective function has to be created. ","category":"page"},{"location":"posterior_background/#Background:-Posterior-probability","page":"Background","title":"Background: Posterior probability","text":"","category":"section"},{"location":"posterior_background/","page":"Background","title":"Background","text":"The posterior objective (and the Log posterior objective which is numerically favorable) allows to define more general objective functions. Form a Bayesian perspective, one is interested in the probability density for a particular parameter lambda given the data x_i_i=1^N y_i_i=1^N and the model m(xlambda):","category":"page"},{"location":"posterior_background/","page":"Background","title":"Background","text":"p(lambda mid x_i_i=1^N y_i_i=1^N m)","category":"page"},{"location":"posterior_background/#Applying-Bayes'-theorem","page":"Background","title":"Applying Bayes' theorem","text":"","category":"section"},{"location":"posterior_background/","page":"Background","title":"Background","text":"Using Bayes' theorem, the probability density can be rewritten as:","category":"page"},{"location":"posterior_background/","page":"Background","title":"Background","text":"p(lambda mid x_i_i=1^N y_i_i=1^N m) = fracell(y_i_i=1^N mid x_i_i=1^N  lambda m )cdot p_0(lambdamid x_i_i=1^N m)p(y_i_i=1^N mid x_i_i=1^N  m)","category":"page"},{"location":"posterior_background/","page":"Background","title":"Background","text":"The denominator is but a normalization constant, that does not depend on lambda, i.e. can be ignored for optimization problems (and MCMC sampling):","category":"page"},{"location":"posterior_background/","page":"Background","title":"Background","text":"p(lambda mid x_i_i=1^N y_i_i=1^N m) propto ell(y_i_i=1^N mid x_i_i=1^N  lambda m )cdot p_0(lambdamid x_i_i=1^N m)","category":"page"},{"location":"posterior_background/","page":"Background","title":"Background","text":"Because of the proportionality, one may refer to the right hand side as unnormalized posterior.","category":"page"},{"location":"posterior_background/","page":"Background","title":"Background","text":"ell is a proper probability distribution for y_i_i=1^N given x_i_i=1^N lambda m. However, it can also be regarded as function of lambda, for fixed x_i_i=1^N, y_i_i=1^N and m (which is needed, since the data is fixed, but different parameters need to be tested for model fitting). In this case, one calls it the likelihood function of lambda. It is no longer a proper probability density (still positive but no longer normalized).\np_0 is the so called prior distribution. It determines the probability of the parameters, before the data was obtained. This is sometimes called belief in parameters or initial knowledge.","category":"page"},{"location":"posterior_background/","page":"Background","title":"Background","text":"default: The prior and objectivity\nA common critique is, that the prior is not objective. While the choice of prior can be subjective, it must be explicitly stated making all assumptions transparent. This allows for an objective comparison of the different approaches.In fact, there are two common types of priors in least squares fitting.p_0(lambdamid x_i_i=1^N m) = 1, i.e. a uniform prior. Since one usually uses a computer, there is a largest number b infty and a smallest number a  -infty that the computer can use. Then one may choose the uniform distribution p_0(lambda mid x_i_i=1^N m) = frac1b-a. Sine the posterior probability is only considered up to proportionality, one can simply use p_0(lambdamid x_i_i=1^N m) = 1. This leads to a maximum likelihood objective.\nIn ill-defined problems, it is common practice to use some kind of regularization. In some cases, these regularizations correspond to certain priors. For example, the Tikhonov regularization essentially uses the prior p_0(lambdamid x_i_i=1^N m) propto exp(-Gamma lambda ^2).","category":"page"},{"location":"posterior_background/#Independent-data-points","page":"Background","title":"Independent data points","text":"","category":"section"},{"location":"posterior_background/","page":"Background","title":"Background","text":"A common assumption is that the data points are independent. While this is not a necessity, writing general likelihood functions is usually not trivial. If the data points are independent, the likelihood function becomes a product of likelihood functions for the individual data point likelihoods:","category":"page"},{"location":"posterior_background/","page":"Background","title":"Background","text":"ell(y_i_i=1^N mid lambda x_i_i=1^N m ) = prod_i=1^N ell_i(y_imid lambda x_i m)","category":"page"},{"location":"posterior_background/","page":"Background","title":"Background","text":"Note that the likelihoods can differ for the different data points, denoted by ell_i here.  Thus the posterior probability / the objective function becomes","category":"page"},{"location":"posterior_background/","page":"Background","title":"Background","text":"p(lambda mid x_i_i=1^N y_i_i=1^N m) propto  p_0(lambdamid x_i_i=1^N m) prod_i_1^n ell_i(y_imid lambda x_im)","category":"page"},{"location":"posterior_background/","page":"Background","title":"Background","text":"In general, x_i is only the measured value, while the true value mathcalX_i is unknown. If the distribution p_i(mathcalX_imid lambda x_i m) is known, marginalization can be used to express the likelihood","category":"page"},{"location":"posterior_background/","page":"Background","title":"Background","text":"ell(y_imid lambda x_i m) = int ell(y_i mid mathcalX_i lambda x_i m)cdot p(mathcalX_imid lambda x_im)  dmathcalX_i","category":"page"},{"location":"posterior_background/","page":"Background","title":"Background","text":"It can happen, that this integral can only be solved numerically. Since this is computationally expensive, and needs to be redone for every new value of lambda, the resulting posterior distribution is often not suited for optimization/sampling purposes. Another approach could be data-augmentation, e.g. to sample ell(y_i mathcalX_imid lambda x_i m), which is not the scope of this package.","category":"page"},{"location":"posterior_background/","page":"Background","title":"Background","text":"The likelihood ell(y_imid mathcalX_i lambda x_i m) is essentially given by the probability distribution q_i(y_imid mathcalY_i) to measure y_i when the true value is mathcalY_i, since mathcalY_i = m(mathcalX_ilambda) by assumption of the model:","category":"page"},{"location":"posterior_background/","page":"Background","title":"Background","text":"ell(y_i mid mathcalX_i lambda x_i m) = q_i(y_imid m(mathcalX_i lambda))","category":"page"},{"location":"posterior_background/#No-x-uncertainty","page":"Background","title":"No x-uncertainty","text":"","category":"section"},{"location":"posterior_background/","page":"Background","title":"Background","text":"A convenient situation is, when the distinction between x_i and mathcalX_i can be neglected, e.g. because the independent variable can be measured with high precision. Then p_i(mathcalX_imid lambda x_i m) becomes a Dirac distribution, and","category":"page"},{"location":"posterior_background/","page":"Background","title":"Background","text":"ell(y_imid lambda x_i m) = q(y_imid m(x_ilambda))","category":"page"},{"location":"posterior_background/","page":"Background","title":"Background","text":"Hence, the posterior probability reads ","category":"page"},{"location":"posterior_background/","page":"Background","title":"Background","text":"p(lambda mid x_i_i=1^N y_i_i=1^N m) propto  p_0(lambdamid x_i_i=1^N m) prod_i_1^n q_i(y_imid m(x_ilambda))","category":"page"},{"location":"posterior_background/#Retrieving-the-LSQ-objective","page":"Background","title":"Retrieving the LSQ objective","text":"","category":"section"},{"location":"posterior_background/","page":"Background","title":"Background","text":"Using the aforementioned uniform prior p_0(lambdamid x_i_i=1^N m) = 1 and assuming normal distributions for q_i with standard deviations Delta y_i leads to","category":"page"},{"location":"posterior_background/","page":"Background","title":"Background","text":"beginaligned\np(lambda mid x_i_i=1^N y_i_i=1^N m) propto   prod_i_1^n frac1sqrt2piDelta y_iexpleft(- frac(y_i - m(x_ilambda))^22Delta y_iright)  \npropto prod_i_1^n expleft(- frac(y_i - m(x_ilambda))^22Delta y_iright) \n quad = expleft(- sum_i=1^N frac(y_i - m(x_ilambda))^22Delta y_iright)\nendaligned","category":"page"},{"location":"posterior_background/","page":"Background","title":"Background","text":"Maximizing this function is equivalent to minimizing","category":"page"},{"location":"posterior_background/","page":"Background","title":"Background","text":"sum_i=1^N frac(y_i - m(x_ilambda))^22Delta y_i","category":"page"},{"location":"posterior_background/","page":"Background","title":"Background","text":"which is the weighted least squares objective (up to a factor frac12) function (see Background: LSQ).","category":"page"},{"location":"lsq_background/#Background:-LSQ","page":"Background","title":"Background: LSQ","text":"","category":"section"},{"location":"lsq_background/","page":"Background","title":"Background","text":"For data points (x_iy_iDelta y_i)_i=1^N, the (weighted) least squares objective function is","category":"page"},{"location":"lsq_background/","page":"Background","title":"Background","text":"textlsq(lambda) = sum_i=1^N frac(y_i - m(x_ilambda))^2Delta y_i^2","category":"page"},{"location":"lsq_background/","page":"Background","title":"Background","text":"For the standard least squares objective function, one sets Δy_i = 1 for all i = 1ldotsn.","category":"page"},{"location":"lsq_background/","page":"Background","title":"Background","text":"The optimal parameters lambda, given the data (x_iy_iDelta y_i) and the model function m(xlambda), are those that minimize the least squares objective function textlsq(lambda). An explanation for this statement can be found in Background:-Posterior-probability","category":"page"},{"location":"posterior_implementation/#Posterior-probability:-How-to-implement","page":"How to implement","title":"Posterior probability: How to implement","text":"","category":"section"},{"location":"posterior_implementation/","page":"How to implement","title":"How to implement","text":"Consider the data and model from Simple-example:","category":"page"},{"location":"posterior_implementation/","page":"How to implement","title":"How to implement","text":"using FittingObjectiveFunctions, Plots #hide\n\nX = collect(1:10)\nY = [1.0, 1.78, 3.64, 3.72, 5.33, 2.73, 7.52, 9.19, 6.73, 8.95]\nΔY = [0.38, 0.86, 0.29, 0.45, 0.66, 2.46, 0.39, 0.45, 1.62, 1.54]\ndata = FittingData(X,Y,ΔY)\nmodel = ModelFunctions((x,λ)-> λ*x)\n\nnothing #hide","category":"page"},{"location":"posterior_implementation/#Likelihood-functions","page":"How to implement","title":"Likelihood functions","text":"","category":"section"},{"location":"posterior_implementation/","page":"How to implement","title":"How to implement","text":"Observer that no distributions were specified in the constructor for data above. As explained in FittingData, the constructor then defaults to normal distributions.  The likelihood function that corresponds to the weighted least squares objective (see Retrieving the LSQ objective) can thus be obtained by","category":"page"},{"location":"posterior_implementation/","page":"How to implement","title":"How to implement","text":"lsq_likelihood = posterior_objective(data,model)","category":"page"},{"location":"posterior_implementation/","page":"How to implement","title":"How to implement","text":"Like lsq_objective, posterior_objective returns a function that takes the model parameters λ as argument.","category":"page"},{"location":"posterior_implementation/","page":"How to implement","title":"How to implement","text":"Normal distributions are not the only possible y-uncertainty distributions (q_i(y_i mid m(x_ilambda))). For example, one might assume a heavy-tailed distribution for measurements, e.g. a Cauchy distribution:","category":"page"},{"location":"posterior_implementation/","page":"How to implement","title":"How to implement","text":"using Distributions #hide\nplot(x-> pdf(Normal(0,1),x), label = \"Normal(0,1)\") #hide\nplot!(x-> pdf(Cauchy(0,1),x), label = \"Cauchy(0,1)\") #hide","category":"page"},{"location":"posterior_implementation/","page":"How to implement","title":"How to implement","text":"To modify the y-uncertainty distributions, the FittingData object must be redefined (see FittingData). The distributions must have the signature (y,m,Δy):","category":"page"},{"location":"posterior_implementation/","page":"How to implement","title":"How to implement","text":"cauchy(y,m,Δy) = pdf(Cauchy(m,Δy),y)\ndata_cauchy = FittingData(X,Y,ΔY,distributions = cauchy)\nnothing #hide","category":"page"},{"location":"posterior_implementation/","page":"How to implement","title":"How to implement","text":"Now a likelihood function with Cauchy uncertainty distributions can be created:","category":"page"},{"location":"posterior_implementation/","page":"How to implement","title":"How to implement","text":"cauchy_likelihood = posterior_objective(data_cauchy,model)","category":"page"},{"location":"posterior_implementation/","page":"How to implement","title":"How to implement","text":"To compare the shape of the resulting likelihood functions, it is helpful to scale up the cauchy_likelihood, such that the maxima have the same height:","category":"page"},{"location":"posterior_implementation/","page":"How to implement","title":"How to implement","text":"large_scope = plot(lsq_likelihood, xlims = [0.9,1.2], label=\"lsq_likelihood\", legend = :topleft) #hide\nplot!(x-> 16 * cauchy_likelihood(x), label = \"cauchy_likelihood\") #hide\nsmall_scope =  plot(lsq_likelihood, xlims = [1.0725,1.0825], legend = :none) #hide\nplot!(x-> 16 * cauchy_likelihood(x)) #hide\nplot(large_scope,small_scope, layout = (1,2), size = (800,300))#hide","category":"page"},{"location":"posterior_implementation/","page":"How to implement","title":"How to implement","text":"tip: More general distributions\nIt is possible to define a likelihood distribution for each data point. Furthermore, these functions can be general julia functions, i.e. while the input arguments must be (y,m,Δy), they need not be utilized in the function.For example, we can assign normal distributions with σ=1 for the first 5 data points, and Cauchy distributions width sigma_i = Delta y_i for the remaining data points:normal_dists = [(y,m,Δy)-> pdf(Normal(m,1),y) for i in 1:5]\ncauchy_dists = [(y,m,Δy)-> pdf(Cauchy(m,Δy),y) for i in 6:length(Y)]\ndata_example = FittingData(X,Y,ΔY, distributions = vcat(normal_dists...,cauchy_dists...))\nlikelihood = posterior_objective(data_example,model)One application for this flexibility is to use the marginalization formula for the likelihood form section Independent-data-points ","category":"page"},{"location":"posterior_implementation/#Using-priors","page":"How to implement","title":"Using priors","text":"","category":"section"},{"location":"posterior_implementation/","page":"How to implement","title":"How to implement","text":"As described in the remark about the objectivity of priors, one always retrieves the likelihood function, when using a uniform prior (over the range of computer representable numbers). This is in fact what happened in the examples Likelihood-functions, as the full constructor for posterior objectives has an optional argument:","category":"page"},{"location":"posterior_implementation/","page":"How to implement","title":"How to implement","text":"posterior_objective(data::FittingData,model::ModelFunction,prior::Function= λ -> 1)","category":"page"},{"location":"posterior_implementation/","page":"How to implement","title":"How to implement","text":"This means, whenever only the two arguments data and model are used, the last argument defaults to λ-> 1. To specify a certain prior, one just needs to pass it as third argument. ","category":"page"},{"location":"posterior_implementation/","page":"How to implement","title":"How to implement","text":"For example, the theory and/or other measurements could indicate that the true parameter value should be lambda = 1. To implement a normal distribution with mu =1 and sigma = 01 as prior, the third argument must be the pdf function for said normal distribution (here using the build in pdf function from Distributions.jl):","category":"page"},{"location":"posterior_implementation/","page":"How to implement","title":"How to implement","text":"posterior = posterior_objective(data,model,λ-> pdf(Normal(1,0.1),λ))","category":"page"},{"location":"posterior_implementation/","page":"How to implement","title":"How to implement","text":"Again, since the posterior and the likelihood are not properly normalized, rescaling is necessary to compare the shapes of lsq_likelihood and posterior:","category":"page"},{"location":"posterior_implementation/","page":"How to implement","title":"How to implement","text":"large_scope = plot(x-> 10/3.3*lsq_likelihood(x), xlims = [0.9,1.2], label = \"lsq_likelihood\", legend = :topleft) #hide\nplot!(posterior, label = \"posterior\")  #hide\nsmall_scope = plot(x-> 10/3.3*lsq_likelihood(x), xlims = [1.065,1.085], legend = :none) #hide\nplot!(posterior)  #hide\nplot(large_scope,small_scope, layout = (1,2), size = (800,300)) #hide","category":"page"},{"location":"#FittingObjectiveFunctions","page":"FittingObjectiveFunctions","title":"FittingObjectiveFunctions","text":"","category":"section"},{"location":"","page":"FittingObjectiveFunctions","title":"FittingObjectiveFunctions","text":"This is a lightweight package without dependencies to create objective functions for model-fitting. To avoid dependencies, this package does not include optimizers / samplers.","category":"page"},{"location":"#Installation","page":"FittingObjectiveFunctions","title":"Installation","text":"","category":"section"},{"location":"","page":"FittingObjectiveFunctions","title":"FittingObjectiveFunctions","text":"This package is not in the general registry and needs to be installed from the GitHub repository:","category":"page"},{"location":"","page":"FittingObjectiveFunctions","title":"FittingObjectiveFunctions","text":"using Pkg\nPkg.add(url=\"https://github.com/AntibodyPackages/FittingObjectiveFunctions\")","category":"page"},{"location":"#Simple-example","page":"FittingObjectiveFunctions","title":"Simple example","text":"","category":"section"},{"location":"","page":"FittingObjectiveFunctions","title":"FittingObjectiveFunctions","text":"After the installation, the package can be used like any other package:","category":"page"},{"location":"","page":"FittingObjectiveFunctions","title":"FittingObjectiveFunctions","text":"using FittingObjectiveFunctions","category":"page"},{"location":"","page":"FittingObjectiveFunctions","title":"FittingObjectiveFunctions","text":"Consider the following example data-set:","category":"page"},{"location":"","page":"FittingObjectiveFunctions","title":"FittingObjectiveFunctions","text":"using Plots\n\nX = collect(1:10)\nY = [1.0, 1.78, 3.64, 3.72, 5.33, 2.73, 7.52, 9.19, 6.73, 8.95]\nΔY = [0.38, 0.86, 0.29, 0.45, 0.66, 2.46, 0.39, 0.45, 1.62, 1.54]\n\nscatter(X,Y, yerror = ΔY, legend=:none, xlabel = \"X\", ylabel=\"Y\")","category":"page"},{"location":"","page":"FittingObjectiveFunctions","title":"FittingObjectiveFunctions","text":"Before objective functions can be created, the data needs to be summarized in a FittingData object:","category":"page"},{"location":"","page":"FittingObjectiveFunctions","title":"FittingObjectiveFunctions","text":"data = FittingData(X,Y,ΔY)\nnothing #hide","category":"page"},{"location":"","page":"FittingObjectiveFunctions","title":"FittingObjectiveFunctions","text":"Information about the model needs to be summarized in a ModelFunctions object. Here we choose a simple linear model m(xlambda) = lambda x:","category":"page"},{"location":"","page":"FittingObjectiveFunctions","title":"FittingObjectiveFunctions","text":"model = ModelFunctions((x,λ) -> λ*x) \nnothing #hide","category":"page"},{"location":"","page":"FittingObjectiveFunctions","title":"FittingObjectiveFunctions","text":"A weighted least squares objective can be be constructed as follows:","category":"page"},{"location":"","page":"FittingObjectiveFunctions","title":"FittingObjectiveFunctions","text":"lsq = lsq_objective(data,model)\n\nplot(λ-> lsq(λ), legend = :none, xlabel = \"λ\", ylabel = \"lsq\")","category":"page"},{"location":"lsq_implementation/#LSQ:-How-to-implement","page":"How to implement","title":"LSQ: How to implement","text":"","category":"section"},{"location":"lsq_implementation/","page":"How to implement","title":"How to implement","text":"Consider the data and model from Simple-example:","category":"page"},{"location":"lsq_implementation/","page":"How to implement","title":"How to implement","text":"using FittingObjectiveFunctions, Plots #hide\n\nX = collect(1:10)\nY = [1.0, 1.78, 3.64, 3.72, 5.33, 2.73, 7.52, 9.19, 6.73, 8.95]\nΔY = [0.38, 0.86, 0.29, 0.45, 0.66, 2.46, 0.39, 0.45, 1.62, 1.54]\ndata = FittingData(X,Y,ΔY)\nmodel = ModelFunctions((x,λ)-> λ*x)\n\nnothing #hide","category":"page"},{"location":"lsq_implementation/#LSQ:-objective-functions","page":"How to implement","title":"LSQ: objective functions","text":"","category":"section"},{"location":"lsq_implementation/","page":"How to implement","title":"How to implement","text":"The uncertainty-weighted least squares objective function can be obtained as follows:","category":"page"},{"location":"lsq_implementation/","page":"How to implement","title":"How to implement","text":"weighted_lsq = lsq_objective(data,model)\nnothing # hide","category":"page"},{"location":"lsq_implementation/","page":"How to implement","title":"How to implement","text":"lsq_objective returns a function that takes the model parameters λ as argument.","category":"page"},{"location":"lsq_implementation/","page":"How to implement","title":"How to implement","text":"To obtain the standard least squares objective, the errors must be set to 1. Recall the shortened constructor (see FittingData):","category":"page"},{"location":"lsq_implementation/","page":"How to implement","title":"How to implement","text":"data_no_errors = FittingData(X,Y)\nstandard_lsq = lsq_objective(data_no_errors,model)\nw_lsq_plot = plot(weighted_lsq, label = \"weighted lsq\", xlims = [0,2], xlabel = \"λ\", ylabel = \"lsq\") #hide\nplot!(standard_lsq, label = \"standard lsq\", xlims = [0,2], xlabel = \"λ\", ylabel = \"lsq\", color = 2) #hide","category":"page"},{"location":"lsq_implementation/#LSQ:-partial-derivatives-and-gradients","page":"How to implement","title":"LSQ: partial derivatives and gradients","text":"","category":"section"},{"location":"lsq_implementation/#Examples","page":"How to implement","title":"Examples","text":"","category":"section"},{"location":"lsq_implementation/","page":"How to implement","title":"How to implement","text":"Redefine model to obtain analytical derivatives (see ModelFunctions):","category":"page"},{"location":"lsq_implementation/","page":"How to implement","title":"How to implement","text":"model = ModelFunctions((x,λ)->λ*x , partials = [(x,λ)-> x])\n∂_weighted_lsq = lsq_partials(data,model)","category":"page"},{"location":"lsq_implementation/","page":"How to implement","title":"How to implement","text":"∂_weighted_lsq[1](1.1)","category":"page"},{"location":"lsq_implementation/","page":"How to implement","title":"How to implement","text":"Note that lsq_partials returns a vector of abstract functions with λ as argument (one for each partial derivative), even in the 1-dimensional case.","category":"page"},{"location":"lsq_implementation/","page":"How to implement","title":"How to implement","text":"∇_weighted_lsq = lsq_gradient(data,model)","category":"page"},{"location":"lsq_implementation/","page":"How to implement","title":"How to implement","text":"∇_weighted_lsq([0.0],1.1) ","category":"page"},{"location":"lsq_implementation/","page":"How to implement","title":"How to implement","text":"On the other hand lsq_gradient directly returns the gradient function, but with a different signature (grad_vector,λ). The argument grad_vector must be a vector of appropriate type and length, that can be mutated.","category":"page"},{"location":"lsq_implementation/","page":"How to implement","title":"How to implement","text":"info: Why mutation for gradient function?\nIn some optimization algorithms, the gradient function is called multiple times during each iteration. Mutating an array allows to reduce the memory allocation overhead of creating a new gradient array every time.","category":"page"},{"location":"lsq_implementation/#Partial-derivatives","page":"How to implement","title":"Partial derivatives","text":"","category":"section"},{"location":"lsq_implementation/","page":"How to implement","title":"How to implement","text":"@doc lsq_partials #hide","category":"page"},{"location":"lsq_implementation/#Gradient","page":"How to implement","title":"Gradient","text":"","category":"section"},{"location":"lsq_implementation/","page":"How to implement","title":"How to implement","text":"@doc lsq_gradient #hide","category":"page"},{"location":"API/#API","page":"API","title":"API","text":"","category":"section"},{"location":"API/#The-FittingData-type","page":"API","title":"The FittingData type","text":"","category":"section"},{"location":"API/","page":"API","title":"API","text":"FittingData","category":"page"},{"location":"API/#FittingObjectiveFunctions.FittingData","page":"API","title":"FittingObjectiveFunctions.FittingData","text":"mutable struct FittingData\n\nData type for fitting data.\n\nThis struct is only a container to check consistency and is not performance relevant, hence the mutability.\n\nFields\n\nindependent: Array of data points of the independent variable. \ndependent: Array of data points of the dependent variable.\nerrors: Array of measurement errors of the dependent variable.\ndistributions: Distribution for the dependent variable uncertainty. Can be a function or an array of functions (one for each data point). \n\nElements with the same index belong together, i.e. define a measurement: \n\n(independent[i], dependent[i], errors[i], distributions[i])\n\nConstructors\n\nFittingData(X,Y)\nFittingData(X,Y,ΔY;distributions = normal_distribution)\n\nDistributions\n\nThe distributions must have the signature (y,m,Δy), where y is the dependent data point, m is the result of the model function, given the dependent data point and the parameter, and Δy is the error of the dependent data point. By default, a normal distribution is used:\n\n(y,m,Δy) -> exp(-(y-m)^2/(2*Δy^2))/(sqrt(2*pi) * Δy)\n\n\n\n\n\n","category":"type"},{"location":"API/#The-ModelFunctions-type","page":"API","title":"The ModelFunctions type","text":"","category":"section"},{"location":"API/","page":"API","title":"API","text":"ModelFunctions","category":"page"},{"location":"API/#FittingObjectiveFunctions.ModelFunctions","page":"API","title":"FittingObjectiveFunctions.ModelFunctions","text":"mutable struct ModelFunctions\n\nMutable type to collect model functions (and the respective partial derivatives) to construct objective functions from.\n\nThis struct is only a container to check consistency and is not performance relevant, hence the mutability.\n\nFields\n\nmodel: The model function. Must have the signature (x,λ), where x is an independent data point, and λ are the parameters (e.g. an array for multiple parameters).\npartials: Array of partial derivative functions (one for each direction). Must have the same signature (x,λ) as the model function.\n\nConstructor\n\nModelFunctions(model, partials = nothing)\n\nExamples\n\nModelFunctions((x,λ)-> λ*x)\t\n\nModelFunctions((x,λ)-> λ*x, partials = [(x,λ)-> x])\t\n\nModelFunctions((x,λ)-> λ[1]*x+λ[2], partials = [(x,λ)-> x, (x,λ)-> 1])\n\n\n\n\n\n","category":"type"},{"location":"API/","page":"API","title":"API","text":"consistency_check","category":"page"},{"location":"API/#FittingObjectiveFunctions.consistency_check","page":"API","title":"FittingObjectiveFunctions.consistency_check","text":"consistency_check(fitting_data::FittingData,model::ModelFunctions)\n\nTest fitting_data and model, e.g. after mutation. \n\n\n\n\n\nconsistency_check(fitting_data::FittingData,model::ModelFunctions,λ)\n\nTest if all functions can be evaluated with the parameter λ. Also, test fitting_data and model, e.g. after mutation. \n\n\n\n\n\n","category":"function"},{"location":"API/#Least-squares-objective","page":"API","title":"Least squares objective","text":"","category":"section"},{"location":"API/","page":"API","title":"API","text":"lsq_objective","category":"page"},{"location":"API/#FittingObjectiveFunctions.lsq_objective","page":"API","title":"FittingObjectiveFunctions.lsq_objective","text":"lsq_objective(data::FittingData,model::ModelFunctions)\n\nReturn the least squares objective function lsq(λ).\n\nAnalytical expression\n\nindependent data points x_i\ndependent data points y_i\nerrors Δy_i (defaulting to 1)\nmodel function m\n\ntextlsq(lambda) = sum_i=1^N frac(y_i - m(x_ilambda))^2Delta y_i^2\n\n\n\n\n\n","category":"function"},{"location":"API/","page":"API","title":"API","text":"lsq_partials","category":"page"},{"location":"API/#FittingObjectiveFunctions.lsq_partials","page":"API","title":"FittingObjectiveFunctions.lsq_partials","text":"lsq_partials(data::FittingData,model::ModelFunctions)\n\nReturn the partial derivatives w.r.t. the parameters [∂_1 ob(λ),…,∂_n ob(λ)] of the least squares objective function ob(λ).\n\nThe partial derivatives (∂_μ m)(x,λ) of the model function must be specified in the ModelFunctions object model.\n\nAnalytical expression\n\nindependent data points: x_i\ndependent data points: y_i\nerrors: Δy_i (defaulting to 1)\nmodel function: m\npartial derivatives of model function in λ: ∂_μ m\n\npartial_mu textlsq(lambda) = sum_i=1^N frac 2 cdot (m(x_ilambda) - y_i) cdot (partial_mu m)((x_ilambda)) Delta y_i^2\n\n\n\n\n\n","category":"function"},{"location":"API/","page":"API","title":"API","text":"lsq_gradient","category":"page"},{"location":"API/#FittingObjectiveFunctions.lsq_gradient","page":"API","title":"FittingObjectiveFunctions.lsq_gradient","text":"lsq_gradient(data::FittingData,model::ModelFunctions)\n\nReturn the gradient function grad!(gradient,λ) for the least squares objective function ob(λ). The gradient function grad! mutates (for performance) and returns the gradient. The elements of gradient do not matter, but the type and length must fit.\n\nThe partial derivatives (∂_μ m)(x,λ) of the model function must be specified in the ModelFunctions object model.\n\nAnalytical expression\n\nindependent data points: x_i\ndependent data points: y_i\nerrors: Δy_i (defaulting to 1)\nmodel function: m\npartial derivatives of model function in λ: ∂_μ m\n\nnabla textlsq(lambda) =  sum_mu  sum_i=1^N frac 2 cdot (m(x_ilambda) - y_i) cdot (partial_mu m)((x_ilambda)) Delta y_i^2  vece_mu\n\n\n\n\n\n","category":"function"},{"location":"API/#Posterior-objective","page":"API","title":"Posterior objective","text":"","category":"section"},{"location":"API/","page":"API","title":"API","text":"posterior_objective","category":"page"},{"location":"API/#FittingObjectiveFunctions.posterior_objective","page":"API","title":"FittingObjectiveFunctions.posterior_objective","text":"posterior_objective(data::FittingData, model::Function,distribution::Function, prior = λ-> 1)\n\nReturn the unnormalized posterior density function p(λ).\n\nUsing the default prior λ-> 1, e.g. py passing only the first two arguments, leads to the likelihood objective for a maximum likelihood fit.\n\nAnalytical expression\n\nindependent data points x_i\ndependent data points y_i\nerrors Δy_i (defaulting to 1)\nmodel function m\ndistributions: p_i\nprior distribution: p_0\n\np(lambda) = p_0(lambda) cdot prod_i=1^N p_i(y_im(x_ilambda)Delta y_i)\n\n\n\n\n\n","category":"function"},{"location":"API/","page":"API","title":"API","text":"log_posterior_objective","category":"page"},{"location":"API/#FittingObjectiveFunctions.log_posterior_objective","page":"API","title":"FittingObjectiveFunctions.log_posterior_objective","text":"log_posterior_objective(data::FittingData,model::ModelFunctions, log_prior::Function = log_uniform_prior)\n\nReturn the logarithmic posterior density function L_p(λ). The model distributions and log-prior are assumed to be already in logarithm form.\n\nUsing the default prior, e.g. py passing only the first two arguments, leads to the logarithmic likelihood objective for a maximum likelihood fit.\n\nAnalytical expression\n\nindependent data points x_i\ndependent data points y_i\nerrors Δy_i (defaulting to 1)\nmodel function m\nlogarithmic distributions: L_i\nlogarithmic prior distribution: L_0\n\nL_p(lambda) = L_0(lambda) + sum_i=1^N L_i(y_im(x_ilambda)Delta y_i)\n\n\n\n\n\n","category":"function"},{"location":"API/","page":"API","title":"API","text":"log_posterior_partials","category":"page"},{"location":"API/#FittingObjectiveFunctions.log_posterior_partials","page":"API","title":"FittingObjectiveFunctions.log_posterior_partials","text":"log_posterior_partials(data::FittingData,model::ModelFunctions, log_distribution_derivatives, prior_partials::Union{Nothing,AbstractArray{F,N}} = nothing) where {F <: Function, N}\n\nReturn the partial derivatives w.r.t. the parameters [∂_1 L_p(λ),…,∂_n L_p(λ)] of the log-posterior function L_p(λ).\n\nThe partial derivatives (∂_μ m)(x,λ) of the model function must be specified in the ModelFunctions object model.\nThe log_distribution_derivatives can either be a function (same derivative for all i), or an array of functions (∂_m L_i)(y,m,Δy)\nThe prior_partials can either be a nothing (for the log-likelihood), or an array of functions (∂_μ L_0)(λ).\n\nAnalytical expression\n\nindependent data points x_i\ndependent data points y_i\nerrors Δy_i (defaulting to 1)\nmodel function m\nlogarithmic distributions: L_i\nlogarithmic prior distribution: L_0\npartial derivatives of model function in λ: ∂_μ m\npartial derivatives of the log-distributions in m: ∂_m L_i\npartial derivatives of the log-prior in λ: ∂_μ L_0\n\npartial_mu L_p(lambda) = partial_mu L_0(lambda) + sum_i=1^N  (partial_m L_i)(y_i m(x_ilambda) Delta y_i)cdot (partial_mu m)(x_ilambda)\n\n\n\n\n\n","category":"function"},{"location":"API/","page":"API","title":"API","text":"log_posterior_gradient","category":"page"},{"location":"API/#FittingObjectiveFunctions.log_posterior_gradient","page":"API","title":"FittingObjectiveFunctions.log_posterior_gradient","text":"log_posterior_gradient(data::FittingData,model::ModelFunctions, log_distribution_derivatives, prior_partials::Union{Nothing,AbstractArray{F,N}} = nothing) where {F <: Function, N}\n\nReturn the gradient function grad!(gradient,λ) for the log-posterior function L_p(λ). The gradient function grad! mutates (for performance) and returns the gradient. The elements of gradient do not matter, but the type and length must fit.\n\nThe partial derivatives (∂_μ m)(x,λ) of the model function must be specified in the ModelFunctions object model.\nThe log_distribution_derivatives can either be a function (same derivative for all i), or an array of functions (∂_m L_i)(y,m,Δy)\nThe prior_gradient can either be a nothing (for the log-likelihood), or a gradient mutating function of functions ∇L_0!(gradient,λ).\n\nAnalytical expression\n\nindependent data points x_i\ndependent data points y_i\nerrors Δy_i (defaulting to 1)\nmodel function m\nlogarithmic distributions: L_i\nlogarithmic prior distribution: L_0\npartial derivatives of model function in λ: ∂_μ m\npartial derivatives of the log-distributions in m: ∂_m L_i\nresulting/mutated log-prior gradient in λ: ∇ L_0\n\nnabla L_p(lambda) = nabla L_0(lambda) + sum_mu sum_i=1^N  (partial_m L_i)(y_i m(x_ilambda) Delta y_i)cdot (partial_mu m)(x_ilambda)  vece_mu\n\n\n\n\n\n","category":"function"}]
}
